{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":6068,"sourceType":"modelInstanceVersion","modelInstanceId":4689}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n\nThe BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n\n**DistilBERT model** is a distilled form of the **BERT** model. The size of a BERT model was reduced by 40% via knowledge distillation during the pre-training phase while retaining 97% of its language understanding abilities and being 60% faster.\n\n\n\n![BERT Architecture](https://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png)\n\n\n","metadata":{}},{"cell_type":"code","source":"!pip install keras-core --upgrade\n!pip install -q keras-nlp --upgrade\n\n\n# This sample uses Keras Core, the multi-backend version of Keras.\n# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-20T04:49:11.784662Z","iopub.execute_input":"2024-06-20T04:49:11.784955Z","iopub.status.idle":"2024-06-20T04:49:37.397339Z","shell.execute_reply.started":"2024-06-20T04:49:11.784929Z","shell.execute_reply":"2024-06-20T04:49:37.396093Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras-core\n  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.23.5)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-core) (13.4.2)\nCollecting namex (from keras-core)\n  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core) (3.9.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.1.8)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.15.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.0)\nInstalling collected packages: namex, keras-core\nSuccessfully installed keras-core-0.1.7 namex-0.0.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom transformers import TFDistilBertModel, DistilBertTokenizer\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T04:52:35.067304Z","iopub.execute_input":"2024-06-20T04:52:35.067716Z","iopub.status.idle":"2024-06-20T04:52:35.075758Z","shell.execute_reply.started":"2024-06-20T04:52:35.067672Z","shell.execute_reply":"2024-06-20T04:52:35.074748Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"TensorFlow version: 2.12.0\nKerasNLP version: 0.12.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-06-20T04:53:29.873662Z","iopub.execute_input":"2024-06-20T04:53:29.874584Z","iopub.status.idle":"2024-06-20T04:53:41.125804Z","shell.execute_reply.started":"2024-06-20T04:53:29.874545Z","shell.execute_reply":"2024-06-20T04:53:41.124774Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.11.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertModel, DistilBertTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-06-20T04:55:42.009344Z","iopub.execute_input":"2024-06-20T04:55:42.010059Z","iopub.status.idle":"2024-06-20T04:55:42.013994Z","shell.execute_reply.started":"2024-06-20T04:55:42.010031Z","shell.execute_reply":"2024-06-20T04:55:42.012943Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Load the Disaster Tweets\nLet's have a look at the train and test dataset.\n\nThey contain:\n- id\n- keyword: A keyword from that tweet (although this may be blank!)\n- location: The location the tweet was sent from (may also be blank)\n- text: The text of a tweet\n- target: 1 if the tweet is a real disaster or 0 if not","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T04:55:43.816877Z","iopub.execute_input":"2024-06-20T04:55:43.817568Z","iopub.status.idle":"2024-06-20T04:55:43.884924Z","shell.execute_reply.started":"2024-06-20T04:55:43.817536Z","shell.execute_reply":"2024-06-20T04:55:43.884037Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Training Set Shape = (7613, 5)\nTraining Set Memory Usage = 0.29 MB\nTest Set Shape = (3263, 4)\nTest Set Memory Usage = 0.10 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T04:55:45.295333Z","iopub.execute_input":"2024-06-20T04:55:45.295813Z","iopub.status.idle":"2024-06-20T04:55:45.317120Z","shell.execute_reply.started":"2024-06-20T04:55:45.295767Z","shell.execute_reply":"2024-06-20T04:55:45.316223Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T13:39:37.233356Z","iopub.execute_input":"2024-06-18T13:39:37.234126Z","iopub.status.idle":"2024-06-18T13:39:37.243908Z","shell.execute_reply.started":"2024-06-18T13:39:37.234088Z","shell.execute_reply":"2024-06-18T13:39:37.243077Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Explore the dataset","metadata":{}},{"cell_type":"code","source":"df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\ndf_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n\nprint(\"Train Length Stat\")\nprint(df_train[\"length\"].describe())\nprint()\n\nprint(\"Test Length Stat\")\nprint(df_test[\"length\"].describe())","metadata":{"execution":{"iopub.status.busy":"2024-06-20T04:55:48.646913Z","iopub.execute_input":"2024-06-20T04:55:48.647720Z","iopub.status.idle":"2024-06-20T04:55:48.677933Z","shell.execute_reply.started":"2024-06-20T04:55:48.647658Z","shell.execute_reply":"2024-06-20T04:55:48.676902Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Train Length Stat\ncount    7613.000000\nmean      101.037436\nstd        33.781325\nmin         7.000000\n25%        78.000000\n50%       107.000000\n75%       133.000000\nmax       157.000000\nName: length, dtype: float64\n\nTest Length Stat\ncount    3263.000000\nmean      102.108183\nstd        33.972158\nmin         5.000000\n25%        78.000000\n50%       109.000000\n75%       134.000000\nmax       151.000000\nName: length, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If you want to know more information about the data, you can grab useful information [here](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n\nNote that all the tweets are in english.","metadata":{}},{"cell_type":"markdown","source":"# Preprocess the data","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_TRAINING_EXAMPLES = df_train.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n\nEPOCHS = 2\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-06-20T04:55:50.424040Z","iopub.execute_input":"2024-06-20T04:55:50.424873Z","iopub.status.idle":"2024-06-20T04:55:50.430776Z","shell.execute_reply.started":"2024-06-20T04:55:50.424829Z","shell.execute_reply":"2024-06-20T04:55:50.429731Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_train[\"text\"]\ny = df_train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n\nX_test = df_test[\"text\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-20T04:55:52.362894Z","iopub.execute_input":"2024-06-20T04:55:52.363530Z","iopub.status.idle":"2024-06-20T04:55:52.383503Z","shell.execute_reply.started":"2024-06-20T04:55:52.363496Z","shell.execute_reply":"2024-06-20T04:55:52.382690Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Load a DistilBERT model from Keras NLP\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n\nThe BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n\nWe will choose DistilBERT model.that learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n\nIt has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n\nSpecifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT.","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertModel, DistilBertTokenizer\nimport keras_nlp\nfrom tensorflow import keras\n# Load a DistilBERT model.\npreset= \"distil_bert_base_en_uncased\"\n\n# Use a shorter sequence length.\npreprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n                                                                   sequence_length=160,\n                                                                   name=\"preprocessor_4_tweets\"\n                                                                  )\n\n# Pretrained classifier.\nclassifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n                                                               preprocessor = preprocessor, \n                                                               num_classes=2)\n\nclassifier.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:58:06.889857Z","iopub.execute_input":"2024-06-22T18:58:06.890099Z","iopub.status.idle":"2024-06-22T18:58:19.983656Z","shell.execute_reply.started":"2024-06-22T18:58:06.890076Z","shell.execute_reply":"2024-06-22T18:58:19.982443Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFDistilBertModel, DistilBertTokenizer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load a DistilBERT model.\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_nlp'"],"ename":"ModuleNotFoundError","evalue":"No module named 'keras_nlp'","output_type":"error"}]},{"cell_type":"markdown","source":"# Train your own model, fine-tuning BERT","metadata":{}},{"cell_type":"code","source":"# Compile\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n    optimizer=keras.optimizers.Adam(1e-5),\n    metrics= [\"accuracy\"]  \n)\n\n# Fit\nhistory = classifier.fit(x=X_train,\n                         y=y_train,\n                         batch_size=BATCH_SIZE,\n                         epochs=EPOCHS, \n                         validation_data=(X_val, y_val)\n                        )","metadata":{"execution":{"iopub.status.busy":"2023-07-31T20:01:37.145916Z","iopub.execute_input":"2023-07-31T20:01:37.146295Z","iopub.status.idle":"2023-07-31T20:04:19.910429Z","shell.execute_reply.started":"2023-07-31T20:01:37.146259Z","shell.execute_reply":"2023-07-31T20:04:19.909408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def displayConfusionMatrix(y_true, y_pred, dataset):\n    disp = ConfusionMatrixDisplay.from_predictions(\n        y_true,\n        np.argmax(y_pred, axis=1),\n        display_labels=[\"Not Disaster\",\"Disaster\"],\n        cmap=plt.cm.Blues\n    )\n\n    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n    f1_score = tp / (tp+((fn+fp)/2))\n\n    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-31T20:04:19.913491Z","iopub.execute_input":"2023-07-31T20:04:19.91385Z","iopub.status.idle":"2023-07-31T20:04:19.923256Z","shell.execute_reply.started":"2023-07-31T20:04:19.913815Z","shell.execute_reply":"2023-07-31T20:04:19.922094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = classifier.predict(X_train)\n\ndisplayConfusionMatrix(y_train, y_pred_train, \"Training\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T20:04:19.924738Z","iopub.execute_input":"2023-07-31T20:04:19.925162Z","iopub.status.idle":"2023-07-31T20:05:04.197288Z","shell.execute_reply.started":"2023-07-31T20:04:19.925118Z","shell.execute_reply":"2023-07-31T20:05:04.19632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_val = classifier.predict(X_val)\n\ndisplayConfusionMatrix(y_val, y_pred_val, \"Validation\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T20:05:04.198688Z","iopub.execute_input":"2023-07-31T20:05:04.199632Z","iopub.status.idle":"2023-07-31T20:05:12.995278Z","shell.execute_reply.started":"2023-07-31T20:05:04.199591Z","shell.execute_reply":"2023-07-31T20:05:12.994338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate the submission file \n\nFor each tweets in the test set, we predict if the given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nThe `submission.csv` file uses the following format:\n`id,target`","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-31T20:05:12.999248Z","iopub.execute_input":"2023-07-31T20:05:13.000332Z","iopub.status.idle":"2023-07-31T20:05:13.023512Z","shell.execute_reply.started":"2023-07-31T20:05:13.000292Z","shell.execute_reply":"2023-07-31T20:05:13.022432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T20:05:13.025189Z","iopub.execute_input":"2023-07-31T20:05:13.025972Z","iopub.status.idle":"2023-07-31T20:05:28.765372Z","shell.execute_reply.started":"2023-07-31T20:05:13.025933Z","shell.execute_reply":"2023-07-31T20:05:28.764404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-31T20:05:28.766701Z","iopub.execute_input":"2023-07-31T20:05:28.767064Z","iopub.status.idle":"2023-07-31T20:05:28.787427Z","shell.execute_reply.started":"2023-07-31T20:05:28.767029Z","shell.execute_reply":"2023-07-31T20:05:28.786399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T20:05:28.788974Z","iopub.execute_input":"2023-07-31T20:05:28.789378Z","iopub.status.idle":"2023-07-31T20:05:28.803925Z","shell.execute_reply.started":"2023-07-31T20:05:28.78933Z","shell.execute_reply":"2023-07-31T20:05:28.803021Z"},"trusted":true},"execution_count":null,"outputs":[]}]}